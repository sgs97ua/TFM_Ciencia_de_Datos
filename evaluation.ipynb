{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación estrategias\n",
    "\n",
    "En este notebook se encuentra el código utilizado para evaluar las diferentes estrategias basadas en diferentes formas de combinación detalladas en la memoria del trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gplsi/anaconda3/envs/GeoPrueba/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-03 12:34:52.599469: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-03 12:34:52.603411: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-03 12:34:52.603420: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from refined.inference.processor import Refined\n",
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Set\n",
    "from typing import Iterable\n",
    "from refined.data_types.doc_types import Doc\n",
    "from refined.data_types.modelling_types import BatchedElementsTns\n",
    "from refined.utilities.preprocessing_utils import convert_doc_to_tensors\n",
    "from refined.data_types.base_types import Span\n",
    "from refined.utilities.preprocessing_utils import pad\n",
    "from refined.data_types.modelling_types import ModelReturn\n",
    "from collections import defaultdict\n",
    "from refined.utilities.general_utils import round_list\n",
    "from refined.data_types.base_types import Entity\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de modelos y métodos auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = Refined.from_pretrained(model_name='wikipedia_model_with_numbers',\n",
    "                                  entity_set=\"wikidata\")\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large',device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text)->dict:\n",
    "    entities = refined.process_text(text)\n",
    "    entities_dicc = {}\n",
    "    \n",
    "    for ent in entities:\n",
    "        entities_dicc[ent.text] = ent\n",
    "\n",
    "    return entities_dicc\n",
    "\n",
    "def get_entity_from_dicc(dictionary:dict,term:str):\n",
    "    entity = dictionary.get(term)\n",
    "    if entity is None:\n",
    "        for key in dictionary:\n",
    "            if key in term or term in key:\n",
    "                entity = dictionary.get(key)\n",
    "                break\n",
    "    \n",
    "    return entity\n",
    "\n",
    "\n",
    "def get_entity_id(entity):\n",
    "    if entity.predicted_entity is None:\n",
    "        name = None\n",
    "    else: \n",
    "        name =  entity.predicted_entity.wikidata_entity_id  \n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición estrategias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definción de la estrategia 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_1(text):\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    relations_count = 0\n",
    "    entities_dicc = extract_entities(text)\n",
    "    relationship_list = []\n",
    "    entities = [ get_entity_id(ent)  for ent in  entities_dicc.values()]\n",
    "    entities =  [x  for x in filter(lambda x: x is not None, entities)]\n",
    "\n",
    "    for triplet in extracted_triplets:\n",
    "        ent_head = get_entity_from_dicc(entities_dicc,triplet['head'])\n",
    "        ent_tail = get_entity_from_dicc(entities_dicc,triplet['tail'])\n",
    "        if ent_head is not None and ent_tail is not None:\n",
    "            relation_dicc = {\n",
    "                \"subject\": {\n",
    "                    \"uri\":get_entity_id(ent_head)\n",
    "                },\n",
    "                \"predicate\":{\n",
    "                    \"surfaceform\": triplet['type'],\n",
    "                },\n",
    "                \"object\":{\n",
    "                    \"uri\": get_entity_id(ent_tail)\n",
    "                }\n",
    "            }\n",
    "            relationship_list.append(relation_dicc)\n",
    "\n",
    "    return relationship_list,entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de la estrategia 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_2(text):\n",
    "    \n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "    extracted_triplets = extract_triplets(extracted_text[0])\n",
    "    entities = set()\n",
    "    relationships_list = []\n",
    "    for triplet in extracted_triplets:\n",
    "        aux = f\"\"\"{triplet['head']} - {triplet['type']} -> {triplet['tail']}\"\"\"\n",
    "        entity_linking = refined.process_text(aux)\n",
    "        relation = \"\"\n",
    "        count = 0\n",
    "        subject = None\n",
    "        obj = None\n",
    "        predicate = None\n",
    "        nentities = len(entity_linking)\n",
    "        for ent in entity_linking:\n",
    "            count += 1\n",
    "            if ent.text in triplet['head']:\n",
    "                subject = get_entity_id(ent)\n",
    "                if subject is None:\n",
    "                    break\n",
    "                entities.add(subject)\n",
    "                #relation = f\"\"\"{ent.predicted_entity.wikidata_entity_id}-\"\"\"\n",
    "            elif ent.text in triplet['tail']:\n",
    "                obj = get_entity_id(ent)\n",
    "                if obj is None:\n",
    "                    break\n",
    "                entities.add(obj)\n",
    "                predicate = triplet['type']\n",
    "                #relation += f\"\"\"{triplet['type']}->{ent.predicted_entity.wikidata_entity_id}\"\"\"\n",
    "                relation_dicc = {\n",
    "                    \"subject\":{\n",
    "                        \"uri\":subject\n",
    "                    },\n",
    "                    \"predicate\":{\n",
    "                        \"surfaceform\":predicate\n",
    "                    },\n",
    "                    \"object\":{\n",
    "                        \"uri\":obj\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                relationships_list.append(relation_dicc)\n",
    "\n",
    "                subject = None\n",
    "                obj = None\n",
    "                predicate = None\n",
    "\n",
    "\n",
    "    return relationships_list, list(entities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de la estrategia 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgbuilder import KGBuilder\n",
    "from refined.data_types.base_types import Span\n",
    "def hash_of_span(self)-> int:\n",
    "    \n",
    "    text = self.text if self.text is not None else \"None\"\n",
    "    entity_id = \"ENTITY NONE\"\n",
    "    wikipedia_entity_title = \"ENTITY NONE\"\n",
    "    \n",
    "    if self.predicted_entity is not  None:    \n",
    "        entity_id = self.predicted_entity.wikidata_entity_id if self.predicted_entity.wikidata_entity_id is not None else \"None\"\n",
    "        wikipedia_entity_title = self.predicted_entity.wikipedia_entity_title if self.predicted_entity.wikipedia_entity_title is not None else \"None\"\n",
    "    \n",
    "    \n",
    "    return hash(text + \" \" + entity_id + \" \"+ wikipedia_entity_title)\n",
    "\n",
    "Span.__hash__ = hash_of_span\n",
    "#builder = KGBuilder()\n",
    "#spans,triplets,spans_el_base,spans_re,triplets_base,triplets_base_er = builder.build_graph(\"Toyota Motor Corporation, founded in 1937 by Kiichiro Toyoda, is one of the world's leading automotive manufacturers. The company originated from the Toyoda Automatic Loom Works, which diversified into automobile production under Kiichiro's vision. Toyota's first passenger car, the Model AA, was produced in 1936. Post-World War II, the company faced financial difficulties but rebounded with innovative manufacturing techniques, including Just-In-Time production, which revolutionized the industry. The introduction of the Corolla in 1966 cemented Toyota's reputation for reliability and affordability. In the 21st century, Toyota became a pioneer in hybrid technology with the launch of the Prius in 1997, leading the global shift towards sustainable automotive solutions. Today, Toyota continues to innovate with advancements in electric vehicles, hydrogen fuel cells, and autonomous driving technologies, maintaining its position as a global automotive leader.\")\n",
    "builder = KGBuilder()\n",
    "def method_3(text):\n",
    "    spans,triplets,_,_,_,_ = builder.build_graph(text)\n",
    "    entities = set()\n",
    "    relations_set = set()\n",
    "    relationships_list = []\n",
    "    for span in spans:\n",
    "        id = get_entity_id(span)\n",
    "        if id is not None:\n",
    "            entities.add(id)\n",
    "    \n",
    "    for triplet in triplets:\n",
    "        subject = get_entity_id(triplet['head'])\n",
    "        predicate = triplet['type']\n",
    "        obj = get_entity_id(triplet['tail'])\n",
    "        if subject is not None and obj is not None:\n",
    "            relation_text = subject+predicate+obj\n",
    "            if relation_text not in relations_set:\n",
    "                relations_set.add(relation_text)\n",
    "                relation_dicc = {\n",
    "                            \"subject\":{\n",
    "                                \"uri\":subject\n",
    "                            },\n",
    "                            \"predicate\":{\n",
    "                                \"surfaceform\":predicate\n",
    "                            },\n",
    "                            \"object\":{\n",
    "                                \"uri\":obj\n",
    "                            }\n",
    "                        }\n",
    "                relationships_list.append(relation_dicc)\n",
    "\n",
    "    \n",
    "    return relationships_list,list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'subject': {'uri': 'Q618370'},\n",
       "   'predicate': {'surfaceform': 'continent'},\n",
       "   'object': {'uri': 'Q51'}},\n",
       "  {'subject': {'uri': 'Q2038835'},\n",
       "   'predicate': {'surfaceform': 'continent'},\n",
       "   'object': {'uri': 'Q51'}},\n",
       "  {'subject': {'uri': 'Q2038835'},\n",
       "   'predicate': {'surfaceform': 'part of'},\n",
       "   'object': {'uri': 'Q618370'}},\n",
       "  {'subject': {'uri': 'Q5395951'},\n",
       "   'predicate': {'surfaceform': 'located on terrain feature'},\n",
       "   'object': {'uri': 'Q2038835'}},\n",
       "  {'subject': {'uri': 'Q5192667'},\n",
       "   'predicate': {'surfaceform': 'located on terrain feature'},\n",
       "   'object': {'uri': 'Q2038835'}},\n",
       "  {'subject': {'uri': 'Q5139027'},\n",
       "   'predicate': {'surfaceform': 'mountain range'},\n",
       "   'object': {'uri': 'Q5395951'}}],\n",
       " ['Q4996295',\n",
       "  'Q5192667',\n",
       "  'Q2038835',\n",
       "  'Q618370',\n",
       "  'Q5395951',\n",
       "  'Q51',\n",
       "  'Q5139027'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_3(\"Coburg Peak (, ) is the rocky peak rising to 783 m in Erul Heights on Trinity Peninsula in Graham Land, Antarctica. It is surmounting Cugnot Ice Piedmont to the northeast.\\n\\nThe peak is named after the Bulgarian royal house of Coburg (Saxe-Coburg-Gotha), 1887–1946.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'subject': {'uri': 'Q2038835'},\n",
       "   'predicate': {'surfaceform': 'part of'},\n",
       "   'object': {'uri': 'Q618370'}},\n",
       "  {'subject': {'uri': 'Q2038835'},\n",
       "   'predicate': {'surfaceform': 'continent'},\n",
       "   'object': {'uri': 'Q51'}},\n",
       "  {'subject': {'uri': 'Q618370'},\n",
       "   'predicate': {'surfaceform': 'continent'},\n",
       "   'object': {'uri': 'Q51'}}],\n",
       " ['Q51', 'Q618370', 'Q2038835'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_2(\"Coburg Peak (, ) is the rocky peak rising to 783 m in Erul Heights on Trinity Peninsula in Graham Land, Antarctica. It is surmounting Cugnot Ice Piedmont to the northeast.\\n\\nThe peak is named after the Bulgarian royal house of Coburg (Saxe-Coburg-Gotha), 1887–1946.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'subject': {'uri': 'Q2038835'},\n",
       "   'predicate': {'surfaceform': 'part of'},\n",
       "   'object': {'uri': 'Q618370'}},\n",
       "  {'subject': {'uri': 'Q2038835'},\n",
       "   'predicate': {'surfaceform': 'continent'},\n",
       "   'object': {'uri': 'Q51'}},\n",
       "  {'subject': {'uri': 'Q618370'},\n",
       "   'predicate': {'surfaceform': 'continent'},\n",
       "   'object': {'uri': 'Q51'}}],\n",
       " ['Q5139027',\n",
       "  'Q5395951',\n",
       "  'Q2038835',\n",
       "  'Q618370',\n",
       "  'Q51',\n",
       "  'Q5192667',\n",
       "  'Q219',\n",
       "  'Q4996295'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_1(\"Coburg Peak (, ) is the rocky peak rising to 783 m in Erul Heights on Trinity Peninsula in Graham Land, Antarctica. It is surmounting Cugnot Ice Piedmont to the northeast.\\n\\nThe peak is named after the Bulgarian royal house of Coburg (Saxe-Coburg-Gotha), 1887–1946.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método en el que se define la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(entities_test,relationships_test,entities,relationships):\n",
    "    n_entities = len(entities_test)\n",
    "    matched_entities = 0\n",
    "    unmatched_entities = 0\n",
    "    for ent in entities:\n",
    "        if ent in entities_test:\n",
    "            matched_entities += 1\n",
    "        else:\n",
    "            unmatched_entities += 1\n",
    "\n",
    "    n_relationships = len(relationships_test)\n",
    "    matched_relationships = 0\n",
    "    unmatched_relationships = 0\n",
    "    for rel in relationships:\n",
    "        matched = False\n",
    "        for test in relationships_test:\n",
    "            if rel[\"subject\"][\"uri\"] == test[\"subject\"][\"uri\"] and rel[\"object\"][\"uri\"] == test[\"object\"][\"uri\"] and rel[\"predicate\"][\"surfaceform\"] ==  test[\"predicate\"][\"surfaceform\"]:\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if matched:\n",
    "            matched_relationships += 1\n",
    "        else:\n",
    "            unmatched_entities += 1\n",
    "                \n",
    "\n",
    "\n",
    "    return n_entities,matched_entities,unmatched_entities,n_relationships,matched_relationships,unmatched_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(method):\n",
    "    file_path = 'en_test.jsonl'\n",
    "\n",
    "    # Reading and processing the JSONL file\n",
    "    n_docs = 300\n",
    "    i = 0\n",
    "\n",
    "    n_entities_list  = []\n",
    "    matched_entities_list  = []\n",
    "    unmatched_entities_list = []\n",
    "    n_relationships_list = []\n",
    "    matched_relationships_list  = []\n",
    "    unmatched_relationships_list = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for obj in reader:\n",
    "            try:\n",
    "                relationships,entities = method(obj['text'])\n",
    "                entities_test = [i[\"uri\"] for i in obj['entities']]\n",
    "                relationshps_test = [i for i in obj['triples']]\n",
    "                n_entities,matched_entities,unmatched_entities,n_relationships,matched_relationships,unmatched_relationships = evaluate(entities_test,relationshps_test,entities,relationships)\n",
    "            except Exception as e:\n",
    "               n_entities,matched_entities,unmatched_entities,n_relationships,matched_relationships,unmatched_relationships = None,None,None,None,None,None\n",
    "            \n",
    "            n_entities_list.append(n_entities)\n",
    "            matched_entities_list.append(matched_entities)\n",
    "            unmatched_entities_list.append(unmatched_entities)\n",
    "            n_relationships_list.append(n_relationships)\n",
    "            matched_relationships_list.append(matched_relationships)\n",
    "            unmatched_relationships_list.append(unmatched_relationships)\n",
    "            i+= 1\n",
    "            print(i)\n",
    "            # You can now process obj as needed\n",
    "            if i > n_docs:\n",
    "                break\n",
    "\n",
    "    data = {\n",
    "        'n_entities': n_entities_list,\n",
    "        'matched_entities': matched_entities_list,\n",
    "        'unmatched_entities': unmatched_entities_list,\n",
    "        'n_relationships': n_relationships_list,\n",
    "        'matched_relationships': matched_relationships_list,\n",
    "        'unmatched_relationships': unmatched_relationships_list\n",
    "    }\n",
    "\n",
    "    stats = pd.DataFrame(data)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_m1 = evaluate_method(method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "stats_m2 = evaluate_method(method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "stats_m3 = evaluate_method(method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_m1.to_csv('./method1.csv',index=False)\n",
    "stats_m2.to_csv('./method2.csv',index=False)\n",
    "stats_m3.to_csv('./method3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stats_m1 = pd.read_csv('./method1.csv')\n",
    "stats_m2 = pd.read_csv('./method2.csv')\n",
    "stats_m3 = pd.read_csv('./method3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_m2 = stats_m2[stats_m3['n_entities'].isna() == False]\n",
    "stats_m1 = stats_m1[stats_m3['n_entities'].isna() == False]\n",
    "stats_m3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(df):\n",
    "    df['accuracy_entities'] = df['matched_entities']/stats_m1[\"n_entities\"]\n",
    "    df['accuracy_relationships'] = df['matched_relationships']/df['n_relationships']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_m1 =extract_metrics(stats_m1)\n",
    "stats_m2 =extract_metrics(stats_m2)\n",
    "stats_m3 =extract_metrics(stats_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_entities</th>\n",
       "      <th>matched_entities</th>\n",
       "      <th>unmatched_entities</th>\n",
       "      <th>n_relationships</th>\n",
       "      <th>matched_relationships</th>\n",
       "      <th>unmatched_relationships</th>\n",
       "      <th>accuracy_entities</th>\n",
       "      <th>accuracy_relationships</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_entities  matched_entities  unmatched_entities  n_relationships  \\\n",
       "0          10.0               7.0                 2.0              3.0   \n",
       "1           6.0               5.0                 5.0              2.0   \n",
       "2          21.0              13.0                16.0              5.0   \n",
       "3           7.0               5.0                 6.0              4.0   \n",
       "4          31.0               9.0                 0.0              1.0   \n",
       "..          ...               ...                 ...              ...   \n",
       "293         7.0               6.0                 3.0              7.0   \n",
       "295         5.0               2.0                 0.0              2.0   \n",
       "297         4.0               0.0                 0.0              1.0   \n",
       "298         4.0               3.0                 1.0              4.0   \n",
       "299         3.0               2.0                 3.0              1.0   \n",
       "\n",
       "     matched_relationships  unmatched_relationships  accuracy_entities  \\\n",
       "0                      3.0                      0.0           0.700000   \n",
       "1                      2.0                      0.0           0.833333   \n",
       "2                      0.0                      0.0           0.619048   \n",
       "3                      3.0                      0.0           0.714286   \n",
       "4                      0.0                      0.0           0.290323   \n",
       "..                     ...                      ...                ...   \n",
       "293                    5.0                      0.0           0.857143   \n",
       "295                    1.0                      0.0           0.400000   \n",
       "297                    0.0                      0.0           0.000000   \n",
       "298                    3.0                      0.0           0.750000   \n",
       "299                    0.0                      0.0           0.666667   \n",
       "\n",
       "     accuracy_relationships  \n",
       "0                  1.000000  \n",
       "1                  1.000000  \n",
       "2                  0.000000  \n",
       "3                  0.750000  \n",
       "4                  0.000000  \n",
       "..                      ...  \n",
       "293                0.714286  \n",
       "295                0.500000  \n",
       "297                0.000000  \n",
       "298                0.750000  \n",
       "299                0.000000  \n",
       "\n",
       "[237 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto promedio de enlazada de entidades por oración para el método 1  0.6160586773512117\n",
      "Tasa de acierto promedio de enlazada de entidades por oración para el método 2  0.27030954250240674\n",
      "Tasa de acierto promedio de enlazada de entidades por oración para el método 3  0.5900300958509763\n"
     ]
    }
   ],
   "source": [
    "print(\"Tasa de acierto promedio de enlazada de entidades por oración para el método 1 \", stats_m1['accuracy_entities'].mean())\n",
    "print(\"Tasa de acierto promedio de enlazada de entidades por oración para el método 2 \", stats_m2['accuracy_entities'].mean())\n",
    "print(\"Tasa de acierto promedio de enlazada de entidades por oración para el método 3 \", stats_m3['accuracy_entities'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto promedio de relaciones por oración para el método 1  0.49592291206215255\n",
      "Tasa de acierto promedio de relaciones por oración para el método 2  0.33931585292344785\n",
      "Tasa de acierto promedio de relaciones por oración para el método 3  0.580664389525149\n"
     ]
    }
   ],
   "source": [
    "print(\"Tasa de acierto promedio de relaciones por oración para el método 1 \", stats_m1['accuracy_relationships'].mean())\n",
    "print(\"Tasa de acierto promedio de relaciones por oración para el método 2 \", stats_m2['accuracy_relationships'].mean())\n",
    "print(\"Tasa de acierto promedio de relaciones por oración para el método 3 \", stats_m3['accuracy_relationships'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de entidades que no coinciden:  3.490566037735849\n",
      "Promedio de entidades que no coinciden:  1.271698113207547\n",
      "Promedio de entidades que no coinciden:  5.158490566037736\n"
     ]
    }
   ],
   "source": [
    "print('Promedio de entidades que no coinciden: ',stats_m1['unmatched_entities'].mean())\n",
    "print('Promedio de entidades que no coinciden: ',stats_m2['unmatched_entities'].mean())\n",
    "print('Promedio de entidades que no coinciden: ',stats_m3['unmatched_entities'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de relaciones que no coinciden:  0.0\n",
      "Promedio de relaciones que no coinciden:  0.0\n",
      "Promedio de relaciones que no coinciden:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('Promedio de relaciones que no coinciden: ',stats_m1['unmatched_relationships'].mean())\n",
    "print('Promedio de relaciones que no coinciden: ',stats_m2['unmatched_relationships'].mean())\n",
    "print('Promedio de relaciones que no coinciden: ',stats_m3['unmatched_relationships'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoPrueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
